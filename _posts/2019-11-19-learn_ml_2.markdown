---
layout: single
title:  "Learn ML Day 2: Supervised vs. Unsupervised"
date:   2019-11-19 10:15:00 -0400
categories: basics machine_learning
---

## Supervised vs. Unsupervised Learning

Supervised:
- Each set of features, have associated outputs. (i.e. labelled dataset)
- Aims to accurately predict the response for future observation (prediction)
- Better understand the relationship between features and output (inference)
- Techniques for supervised approaches:
    - linear regression
    - logistic regression
    - GAM
    - boosting
    - SVM
 
Unsupervised:
- Lacks output variable that can supervise the analysis.
- Seek to understand the relationships between the variables:
    - Cluster Analysis (clustering)
- Examples:
    - market segmentation study (categorize customers to groups)

Semi-supervised Learning:
- For cases where only parts of the dataset is labelled.

## Regression vs. Classification

Classification: prediction of discrete categories (i.e. gender, country)
- logistic regression

Regression: prediction of continuous values (i.e. price, age)
- Least squares

- Estimation of Class Probabilities can be either regression/classification problem.
- K-nearest neighbour can be used for both.
- The choice of regression/classification based on predictor is not important, since they can be encoded before analysis.
 
## Model Accuracy
No free lunch theorem: No one method dominates all others over in possible datasets.

It is important to decide which models to use for a specific given task.

For regression: Mean squared error is a commonly used measure.

\\[ MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2 \\]

Training accuracy is often not important, but the generalization capability on unseen dataset. Therefore, the model should be optimized for testing accuracy.

The U-shape is a fundamental property of any statistical learning. The U-shape occurs between training, test error. 
- As model flexibility increase, training error decreases, but test error might not.
- Small training error, but large test error = **Overfitting**

Since there are usually not many test data, cross-validation is a very important method for estimating test error using the training data.

## Bias-Variance Trade-Off

\\[ E \Bigl(y_0 - \hat{f}(x_0)\Bigr)^2 = \mathrm{Var}(\hat{f}(x_0)) + [\mathrm{Bias}(\hat{f}(x_0))]^2 + \mathrm{Var}(\epsilon) \\]

- The test error can never be lower than irreducible error.
- We opt for method that simultaneously give low variance and low bias.
- Variance of \\(\hat{f}\\) is how much \\(\hat{f}\\) would change if it was estimated using different dataset.
- Ideally Variance of \\(\hat{f}\\) should not change too much from dataset to datsets.
- High variance of \\(\hat{f}\\) means the approximation changes even with small differences in datasets. This could be a sign of overfitting.
- In general, more flexible methods have higher variance.

- Bias of \\(\hat{f}\\) refers to error introduced by approximating a real-life problem.
- Bias results from assumption of too simplistic model to solve a problem.
- Generally, more flexible methods have lower bias.

In general, it is hard to explicitly calculate test bias, variance.

## Classification
In classification, we do not deal with continuous output variables, and thus we need to use discrete error functions. For example:

\\[ \frac{1}{n} \sum_{i=1}^{n}I(y_i \neq \hat{y}_i) \\]

The indicator function outputs 1, if the prediction did not match the ground truth output class. Thus, the above error function is a ratio of incorrect prediction over total output length.

**The Bayes Classifier**: We can use estimate based on conditional probability between the features and the output:

\\[ P(Y = j\ \|\ X = x_0) \\]

This is the conditional probability that \\(Y\\) is certain class \\(j\\), given the predictor vector \\(x_0\\).

The byes classifier's prediction is determined by the Bayes decision boundary.

Bayes error rate is the lowest possible test error rate. The classifier will pick the class for the sete of features that make the conditional probability the highest. Thus, the Bayes error rate is:

\\[ 1 - E\Bigl( \max_j P(Y = j \|X)\Bigr) \\]

The Bayes error rate is analogous to the irreducible error.

**K-Nearest Neighbors**
- For real data, we do not know the conditional distribution of \\(Y \\) given \\(X\\). There for computing the Bayes classifier is impossible. 
- Many methods attempt to estimate the conditional distribution of \\(Y\\) given \\(X\\) and then classify a given observation to the class with the highest estimate probability.

\\[P(Y = j \| X = x_0) = \frac{1}{K}\sum_{i \in \mathcal{N}_0}I(y_i = j)\\]

- \\(\mathcal{N}_0\\) represents the K nearest points to the feature \\(x_0\\)
- The KNN estimate the conditional probability for class j as the fraction of points in \\(\mathcal{N}_0\\).
- The KNN is simple yet a very effective approach to the optimal Bayes classifiers.
- The choice of K (hyperparameter) affect the model. As K grows, the model becomes less flexible and generates smoother boundaries. 
- Thus, with K == 1, the model severely overfits (low train error, high test error).
- With K == 100, the model is not sufficiently flexible. (high train/test error).